# -*- coding: utf-8 -*-
"""Day11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YoKKGXQWec3pbHtEpLu_dB3TrXFEsUNJ
"""

# Task 1 Load & Inspect the Dataset
# 1.Load the dataset
import pandas as pd
df=pd.read_csv("Bengaluru_House_Data.csv")
#2.Display first 5 rows
print("First 5 Rows")
display(df.head())
#3.Display last 5 rows
print("Last 5 Rows")
display(df.tail())
# 4.Print dataset shape
print("Dataset Shape (rows, columns):",df.shape)
#5.Dataset information
print("Dataset Info")
df.info()
#6.Print data types of all columns
print("Data Types of Columns")
print(df.dtypes)

# Check if any null values exist
print("Any Null Values Present?:")
print(df.isnull().any())
print("\nNumber of Null Values in Each Column")
print(df.isnull().sum())
print("\nTotal Null Values in the Dataset")
print(df.isnull().sum().sum())

# Task 2 – Data Cleaning
import numpy as np
#1.Identify & count missing values in each column
print("Missing Values in Each Column")
print(df.isnull().sum())
#2.Drop irrelevant columns for price analysis
columns_to_drop=['society']
df=df.drop(columns=columns_to_drop,errors='ignore')
print("\nDropped columns:",columns_to_drop)
#3.Handle missing values in numeric columns
num_cols=['bath','balcony']
for col in num_cols:
    if col in df.columns:
        median_value=df[col].median()
        df[col]=df[col].fillna(median_value)
#4.Convert the 'total_sqft' column to numeric
def convert_sqft(x):
    try:
        # Case 1: value like "2100 - 2850"
        if '-' in x:
            nums=x.split('-')
            return (float(nums[0].strip()) + float(nums[1].strip())) / 2
        # Case 2: values like "34.46Sq. Meter"
        return float(x)
    except:
        return np.nan
df['total_sqft']=df['total_sqft'].astype(str).apply(convert_sqft)
#Handle new NaN created after conversion
df['total_sqft']=df['total_sqft'].fillna(df['total_sqft'].median())
#5.Remove duplicate rows
df=df.drop_duplicates()
#6.Reset index
df=df.reset_index(drop=True)
print("\nDataset After Cleaning")
df.info()

#IASK -3
#1.Count unique locations
unique_locations=df['location'].nunique()
print("Number of Unique Locations:",unique_locations)
#2.Average price for each location
avg_price_location=df.groupby('location')['price'].mean().sort_values(ascending=False)
print("Average House Price for Each Location")
avg_price_location
#3.Location with highest average price
highest_price_location=avg_price_location.head(1)
print("Location With Highest Average Price")
print(highest_price_location)
#4.Correlation among key numeric features
corr_data=df[['total_sqft', 'bath', 'price']]
print("Correlation Matrix")
print(corr_data.corr())

#Task 4 — Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('seaborn-v0_8')
#1️. Price Distribution — Histogram + KDE
plt.figure(figsize=(10,5))
sns.histplot(df['price'], kde=True, bins=50)
plt.title("Price Distribution")
plt.xlabel("Price (in Lakhs)")
plt.ylabel("Frequency")
plt.show()
#2️.Relationship Between Area and Price — Scatter Plot
plt.figure(figsize=(10,6))
plt.scatter(df['total_sqft'],df['price'],alpha=0.5)
plt.title("Relationship: Total Sqft vs Price")
plt.xlabel("Total Sqft")
plt.ylabel("Price (in Lakhs)")
plt.show()
# 3.Effect of Bathrooms on Price — Box Plot
plt.figure(figsize=(10,5))
sns.boxplot(x=df['bath'], y=df['price'])
plt.title("Effect of Bathrooms on House Price")
plt.xlabel("Number of Bathrooms")
plt.ylabel("Price (in Lakhs)")
plt.show()
#4.Top 10 Most Expensive Locations — Bar Chart
avg_price_location=df.groupby('location')['price'].mean().sort_values(ascending=False)
top10=avg_price_location.head(10)
plt.figure(figsize=(12,6))
sns.barplot(x=top10.values,y=top10.index)
plt.title("Top 10 Most Expensive Locations")
plt.xlabel("Average Price (in Lakhs)")
plt.ylabel("Location")
plt.show()
#5.Correlation Heatmap — Numeric Columns
plt.figure(figsize=(8,5))
numeric_cols=df[['price', 'total_sqft', 'bath', 'balcony']]   # Add more if needed
corr_matrix=numeric_cols.corr()
sns.heatmap(corr_matrix, annot=True,cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

#Task 5 — Export the Cleaned Dataset
df.to_csv("cleaned_house_price_data.csv", index=False)
print("Cleaned dataset saved as cleaned_house_price_data.csv")

#2️. Download the file to your local machine
from google.colab import files
files.download("cleaned_house_price_data.csv")
#3.Save to google drive
from google.colab import drive
drive.mount('/content/drive')

# Save to Drive
df.to_csv("/content/drive/MyDrive/cleaned_house_price_data.csv", index=False)
print("File saved to Google Drive: MyDrive/cleaned_house_price_data.csv")
#4️.Verify by loading the saved file again
df2 = pd.read_csv("cleaned_house_price_data.csv")

print("----- First 5 Rows of Verified File -----")
df2.head()

#✅ Task 1 — Load & Inspect the Dataset (Amazon_electronics.csv)
#1️⃣ Load the dataset
import pandas as pd

df = pd.read_csv("Amazon_electronics.csv")
print("Dataset Loaded Successfully!")

#2️⃣ Display the first 5 rows
print("FIRST 5 ROWS")
df.head()
#3️⃣ Display the last 5 rows
print("LAST 5 ROWS")
df.tail()
#4️⃣ Print dataset shape (rows, columns)
print("Dataset Shape (rows, columns):",df.shape)
#5️⃣ Print dataset info()
print("DATASET INFO")
df.info()
#6️⃣ Show list of unique product categories

print("UNIQUE PRODUCT CATEGORIES")
print(df['category'].unique())
#7️⃣ Identify columns that require cleaning or type conversion
print("NULL VALUES IN EACH COLUMN")
print(df.isnull().sum())
print("\nDATA TYPES OF EACH COLUMN")
print(df.dtypes)

# Task 2 — Clean the Dataset (Amazon Electronics)
import pandas as pd

df = pd.read_csv("Amazon_electronics.csv")
print("Dataset Loaded Successfully!")

# 1️⃣ Identify missing values
print("----- Missing Values in Each Column -----")
print(df.isnull().sum())     # ADDED PRINT STATEMENT

# 2️⃣ Fill missing numeric values (rating, discount_price, year, etc.)
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
for col in numeric_cols:
    df[col] = df[col].fillna(df[col].median())
print("Numeric missing values filled.")

# 3️⃣ Fill missing categorical values (brand, category, user_attr, model_attr) with 'Unknown'
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    df[col] = df[col].fillna("Unknown")
print("Categorical missing values filled.")

# 4️⃣ Remove duplicate rows
df = df.drop_duplicates()
print("Duplicate rows removed.")

# 5️⃣ Convert numeric columns to correct data types
numeric_convert = ['rating', 'item_id', 'user_id', 'year']
for col in numeric_convert:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

df[numeric_convert] = df[numeric_convert].fillna(df[numeric_convert].median())

# Convert timestamp to datetime
if 'timestamp' in df.columns:
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

# 6️⃣ Reset index after cleaning
df.reset_index(drop=True, inplace=True)
print("Index reset.")

# EXTRA: show cleaned dataset summary
print("\n----- Cleaned Dataset Info -----")
print(df.info())

print("\n----- First 5 Rows of Clean Dataset -----")
print(df.head())

# Task 3 — Data Analysis (Amazon Electronics)

# 1️⃣ Top 5 highest-rated products
print("\nTop 5 Highest Rated Products:")
print(df[['item_id', 'brand', 'rating']]
      .sort_values(by='rating', ascending=False)
      .head())

# 2️⃣ Brand with highest number of products
print("\nBrand with Highest Number of Products:")
print(df['brand'].value_counts().head(1))

# 3️⃣ Average rating for each brand
print("\nAverage Rating for Each Brand:")
print(df.groupby('brand')['rating'].mean().sort_values(ascending=False))

# 4️⃣ Products with rating ≥ 4.5
print("\nProducts with Rating ≥ 4.5:")
print(df[df['rating'] >= 4.5][['item_id', 'brand', 'rating']])

# 5️⃣ Category-wise average rating
print("\nCategory-wise Average Rating:")
print(df.groupby('category')['rating'].mean())



#Task 4A — Feature Engineering (Adjusted for Your Dataset)
import pandas as pd
import numpy as np
# Load dataset
df = pd.read_csv("Amazon_electronics.csv")
print("Dataset Loaded Successfully!\n")
# Simulated discount_percent (if actual_price & discount_price existed)
if 'actual_price' in df.columns and 'discount_price' in df.columns:
    df['discount_percent'] = ((df['actual_price'] - df['discount_price']) / df['actual_price']) * 100
else:
    # Placeholder: all zeros
    df['discount_percent'] = 0
# Price category based on actual_price
def price_category(price):
    if price < 500:
        return "Budget"
    elif price < 2000:
        return "Midrange"
    else:
        return "Premium"

if 'actual_price' in df.columns:
    df['price_category'] = df['actual_price'].apply(price_category)
else:
    # Placeholder: all 'Unknown'
    df['price_category'] = "Unknown"
# Popularity score = rating * log(rating_count + 1)
if 'rating_count' in df.columns:
    df['popularity_score'] = df['rating'] * np.log(df['rating_count'] + 1)
else:
    # Placeholder: popularity_score = rating
    df['popularity_score'] = df['rating']
# Display first 5 rows with new columns
print("First 5 rows with new features:")
print(df[['discount_percent', 'price_category', 'popularity_score']].head())

#Task 4B — Filtering Using New Columns (Adjusted)
import pandas as pd
import numpy as np
# Load dataset and assume Task 4A features are created
df = pd.read_csv("Amazon_electronics.csv")
# Simulate features if they do not exist
if 'popularity_score' not in df.columns:
    df['popularity_score'] = df['rating']
if 'discount_percent' not in df.columns:
    df['discount_percent'] = 0
if 'price_category' not in df.columns:
    df['price_category'] = "Unknown"
# 1️⃣ Top 10 highest popularity_score products
top_popular = df.sort_values(by='popularity_score', ascending=False).head(10)
print("Top 10 Highest Popularity Score Products:")
print(top_popular[['item_id', 'brand', 'popularity_score']])
# 2️⃣ All Premium category items with discount_percent > 40%
premium_discount = df[(df['price_category'] == 'Premium') & (df['discount_percent'] > 40)]
print("\nPremium Items with Discount > 40%:")
print(premium_discount[['item_id', 'brand', 'price_category', 'discount_percent']])
# 3️⃣ Filter products with rating >= 4.0, discount_percent 20–50%, popularity_score in top 30%
pop_threshold = df['popularity_score'].quantile(0.7)
filtered_products = df[
    (df['rating'] >= 4.0) &
    (df['discount_percent'].between(20, 50)) &
    (df['popularity_score'] >= pop_threshold)
]
print("\nFiltered Products (High Rating + Discount + Top Popularity):")
print(filtered_products[['item_id', 'brand', 'rating', 'discount_percent', 'popularity_score']])

# Task 4C — Grouping with New Columns (Amazon Electronics Dataset)
import pandas as pd
# Load dataset
df=pd.read_csv("Amazon_electronics.csv")
print("Dataset Loaded Successfully!\n")
# 1️⃣ Average rating per brand
avg_rating_brand=df.groupby('brand')['rating'].mean().sort_values(ascending=False)
print("Average Rating for Each Brand:\n", avg_rating_brand, "\n")
# 2️⃣ Average rating per category
avg_rating_category=df.groupby('category')['rating'].mean().sort_values(ascending=False)
print("Average Rating for Each Category:\n", avg_rating_category, "\n")
# 3️⃣ Category with highest average rating
highest_avg_category=avg_rating_category.head(1)
print("Category with Highest Average Rating:\n", highest_avg_category, "\n")
# 4️⃣ Simulated Revenue Potential per Brand
brand_counts=df.groupby('brand').size()
avg_rating=df.groupby('brand')['rating'].mean()
revenue_potential=avg_rating * brand_counts
revenue_potential=revenue_potential.sort_values(ascending=False)
print("Simulated Revenue Potential per Brand:\n",revenue_potential, "\n")

# Task 5 — Data Visualization
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset and ensure popularity_score exists
df = pd.read_csv("Amazon_electronics.csv")
if 'popularity_score' not in df.columns:
    df['popularity_score'] = df['rating']
# 1️⃣ Rating Distribution (Histogram + KDE)
plt.figure(figsize=(8,5))
sns.histplot(df['rating'], kde=True, bins=20, color='skyblue')
plt.title("Rating Distribution")
plt.xlabel("Rating")
plt.ylabel("Frequency")
plt.show()
# 2️⃣ Relationship between rating and popularity_score (Scatter Plot)
plt.figure(figsize=(8,5))
sns.scatterplot(x='rating', y='popularity_score', data=df, hue='brand', legend=False)
plt.title("Relationship between Rating and Popularity Score")
plt.xlabel("Rating")
plt.ylabel("Popularity Score")
plt.show()
# 3️⃣ Category-wise Average Rating (Bar Plot)
category_avg = df.groupby('category')['rating'].mean().sort_values(ascending=False)
plt.figure(figsize=(10,5))
sns.barplot(x=category_avg.index, y=category_avg.values, palette="viridis")
plt.xticks(rotation=45, ha='right')
plt.title("Category-wise Average Rating")
plt.xlabel("Category")
plt.ylabel("Average Rating")
plt.show()
# 4️⃣ Brand Product Count (Count Plot)
plt.figure(figsize=(12,5))
sns.countplot(x='brand', data=df, order=df['brand'].value_counts().index, palette="coolwarm")
plt.xticks(rotation=90)
plt.title("Number of Products per Brand")
plt.xlabel("Brand")
plt.ylabel("Count")
plt.show()
# 5️⃣ Numeric Feature Correlations (Heatmap)
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
plt.figure(figsize=(8,6))
sns.heatmap(df[numeric_cols].corr(), annot=True, cmap="YlGnBu", fmt=".2f")
plt.title("Correlation Heatmap of Numeric Features")
plt.show()

# 1. Load libraries & Dataset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load Titanic dataset
df = sns.load_dataset('titanic')
df.head()

# 4. Handling Missing Values

# Fill the numeric missing values with the mean
df['age'] = df['age'].fillna(df['age'].mean())

# Fill categorical missing values with mode
df['embarked'] = df['embarked' ].fillna(df['embarked'].mode() [0])

# Drop 'deck' column (too many missing values)
df.drop(columns=['deck' ], inplace= True)

print("Missing values after cleaning: \n", df.isnull().sum().sum())

# 1. Load libraries & Dataset
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load Titanic dataset
df1 = sns.load_dataset('titanic')
print(df1.head())
print(df1.shape)
missing = pd.DataFrame({
'missing_count':df.isnull().sum(),
'missing_percent': (df.isnull().sum()/ len(df)) * 100

})

missing.sort_values(by='missing_percent', ascending=False)

# Missing values analysis

missing = pd.DataFrame({
'missing_count':df.isnull().sum(),
'missing_percent': (df.isnull().sum()/ len(df)) * 100
})
missing.sort_values(by='missing_percent', ascending=False)

# Duplicates & Data type check

print("Duplicate rows: ", df.duplicated().sum())

# Convert some columns to category type
df['class'] = df['class'].astype('category')
df['sex'] = df['sex'].astype('category')
df['embarked'] = df['embarked' ].astype('category')

df.info()

# 6. Univariate analysis - Numerical Fetures
num_cols = df.select_dtypes(include=['float64','int64']).columns
df[num_cols].describe()

# Histograme : Age
sns.histplot(df['age'],bins= 30, kde = True , color= 'skyblue')
plt.title("Age Distribution of Passengers")
plt. show()

# Boxplot : Fare
sns.boxplot(x=df['fare'], color='orange' )
plt.title("Fare Distribution with Outliers")
plt.show()

# Outlier Dection using IQR
Q1 = df['fare'].quantile(0.25)
Q3 = df['fare'].quantile(0.75)
IQR = Q3 - Q1

lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

outliers = df[(df['fare' ] < lower) | (df['fare'] > upper)]
print("Number of outliers in fare: ", len(outliers))

# Outliers
df['fare_capped' ] = df['fare'].clip(lower, upper)
sns.boxplot(x=df['fare_capped' ])
plt.title("Fare after outlier capping")
plt.show()

# Task 1 — Data Loading, Merging & Initial Inspection
import pandas as pd

# ------------------------------
# 1️⃣ Load CSV files
train = pd.read_csv('/content/train.csv')
stores = pd.read_csv('/content/stores.csv')
features = pd.read_csv('/content/features.csv')

print("Datasets Loaded Successfully!\n")

# ------------------------------
# 2️⃣ Merge datasets on Store, Dept, and Date
# Merge train + features on Store & Date
train_features = pd.merge(train, features, on=['Store', 'Date'], how='left')

# Merge with stores on Store
full_df = pd.merge(train_features, stores, on='Store', how='left')

print("Datasets Merged Successfully!\n")
print("Merged Dataset Shape:", full_df.shape)
print("\nFirst 5 rows:")
print(full_df.head())


# ------------------------------
# 3️⃣ Display first and last 10 rows
print("First 10 Rows:")
print(full_df.head(10))

print("\nLast 10 Rows:")
print(full_df.tail(10))

# ------------------------------
# 4️⃣ Print shape, info, and descriptive stats
print("\nDataset Shape:", full_df.shape)

print("\nDataset Info:")
print(full_df.info())

print("\nDescriptive Statistics (Numerical Columns):")
print(full_df.describe())

# ------------------------------
# 5️⃣ Identify column types
numerical_cols = full_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = full_df.select_dtypes(include=['object']).columns.tolist()
date_cols = full_df.select_dtypes(include=['datetime64[ns]']).columns.tolist()  # will convert Date later

print("\nNumerical Columns:", numerical_cols)
print("Categorical Columns:", categorical_cols)
print("Date Columns (before conversion):", date_cols)

# ------------------------------
# 6️⃣ List unique store types and departments
print("\nUnique Store Types:", full_df['Type'].unique())
print("Unique Departments:", full_df['Dept'].unique())

# ------------------------------
# 7️⃣ Identify columns that may require cleaning or type conversion
# For example: Date column to datetime, any numeric columns with missing values
print("\nColumns Requiring Cleaning or Type Conversion:")
print("- 'Date' → convert to datetime")
print("- Check for missing values in numeric columns like 'Weekly_Sales'")
print("- Categorical columns like 'Type' and 'IsHoliday' may need encoding")

# Task 2 — Data Cleaning
import pandas as pd

# Assume full_df is the merged dataset from Task 1
df = full_df.copy()
print("Data Cleaning Started...\n")

# 1️⃣ Identify missing values
missing_values = df.isnull().sum()
print("Missing Values in Each Column:\n", missing_values, "\n")

# 2️⃣ Fill missing numeric values with median
numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']
for col in numeric_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].median())
print("Filled missing numeric values with median.\n")

# 3️⃣ Fill missing markdown fields with mean
markdown_cols = [col for col in df.columns if 'MarkDown' in col]
for col in markdown_cols:
    df[col] = df[col].fillna(df[col].mean())
print("Filled missing markdown fields with mean.\n")

# 4️⃣ Convert Date column to datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
print("Converted 'Date' column to datetime.\n")

# 5️⃣ Remove duplicate rows
df = df.drop_duplicates()
print("Duplicate rows removed.\n")

# 6️⃣ Reset index after cleaning
df.reset_index(drop=True, inplace=True)
print("Index reset after cleaning.\n")

# ✅ Display first 5 rows after cleaning
print("First 5 rows after cleaning:")
print(df.head())

# Task 3 — Outlier Detection & Treatment (Corrected)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assume df_clean is your cleaned dataset from Task 2
# Ensure 'IsHoliday' column exists
if 'IsHoliday_x' in df_clean.columns:
    df_clean['IsHoliday'] = df_clean['IsHoliday_x']
elif 'IsHoliday_y' in df_clean.columns:
    df_clean['IsHoliday'] = df_clean['IsHoliday_y']

# ------------------------------
# 1️⃣ Columns to check for outliers
cols_to_check = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI']

# ------------------------------
# 2️⃣ Visualize outliers using boxplots
plt.figure(figsize=(15, 10))
for i, col in enumerate(cols_to_check):
    plt.subplot(2, 2, i+1)
    sns.boxplot(x=df_clean[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

# ------------------------------
# 3️⃣ Detect outliers using IQR method
def detect_outliers_iqr(data, col):
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = data[(data[col] < lower) | (data[col] > upper)]
    return outliers, lower, upper

# Dictionary to store outlier thresholds
outlier_info = {}

for col in cols_to_check:
    outliers, lower, upper = detect_outliers_iqr(df_clean, col)
    outlier_info[col] = (lower, upper)
    print(f"{col}: {len(outliers)} outliers detected (IQR method)")

# ------------------------------
# 4️⃣ Check if extreme sales spikes occur during holiday weeks
if 'IsHoliday' in df_clean.columns:
    holiday_sales = df_clean[df_clean['IsHoliday'] == True]['Weekly_Sales']
    plt.figure(figsize=(8,5))
    sns.boxplot(x=holiday_sales)
    plt.title("Boxplot of Weekly Sales during Holiday Weeks")
    plt.show()

    weekly_lower, weekly_upper = outlier_info['Weekly_Sales']
    holiday_outliers = holiday_sales[(holiday_sales < weekly_lower) | (holiday_sales > weekly_upper)]
    print(f"Extreme sales during holiday weeks: {len(holiday_outliers)}\n")

# ------------------------------
# 5️⃣ Outlier Treatment
# Temperature, Fuel_Price, CPI → cap all extreme values
for col in ['Temperature', 'Fuel_Price', 'CPI']:
    lower, upper = outlier_info[col]
    df_clean[col] = df_clean[col].clip(lower, upper)

# Weekly_Sales → cap only non-holiday outliers
lower, upper = outlier_info['Weekly_Sales']
df_clean.loc[df_clean['IsHoliday']==False, 'Weekly_Sales'] = \
    df_clean.loc[df_clean['IsHoliday']==False, 'Weekly_Sales'].clip(lower, upper)

print("Outlier treatment applied (capped non-holiday sales, capped numeric features).\n")

# ------------------------------
# Display summary after outlier treatment
print(df_clean.describe())

# Task 4 — Univariate Analysis
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Use cleaned and outlier-treated dataset
df_analysis = df_clean.copy()

# ------------------------------
# 1️⃣ Weekly Sales Distribution (Histogram + KDE)
plt.figure(figsize=(10,5))
sns.histplot(df_analysis['Weekly_Sales'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Weekly Sales')
plt.xlabel('Weekly Sales')
plt.ylabel('Frequency')
plt.show()

# ------------------------------
# 2️⃣ Store Type Distribution (Count Plot)
plt.figure(figsize=(8,5))
sns.countplot(x='Type', data=df_analysis, palette='Set2')
plt.title('Store Type Distribution')
plt.xlabel('Store Type')
plt.ylabel('Count')
plt.show()

# ------------------------------
# 3️⃣ Distribution of Temperature, Fuel Price, CPI, Unemployment
numeric_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']

plt.figure(figsize=(15,10))
for i, col in enumerate(numeric_cols):
    plt.subplot(2,2,i+1)
    sns.histplot(df_analysis[col], kde=True, bins=40, color='orange')
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# ------------------------------
# 4️⃣ Distribution of Weekly Sales during Holiday and Non-Holiday Weeks
plt.figure(figsize=(12,5))

# Holiday Weeks
plt.subplot(1,2,1)
sns.histplot(df_analysis[df_analysis['IsHoliday']==True]['Weekly_Sales'], kde=True, color='red', bins=30)
plt.title('Weekly Sales During Holiday Weeks')
plt.xlabel('Weekly Sales')

# Non-Holiday Weeks
plt.subplot(1,2,2)
sns.histplot(df_analysis[df_analysis['IsHoliday']==False]['Weekly_Sales'], kde=True, color='green', bins=30)
plt.title('Weekly Sales During Non-Holiday Weeks')
plt.xlabel('Weekly Sales')

plt.tight_layout()
plt.show()

# ------------------------------
# 5️⃣ Top 10 Departments by Average Weekly Sales
dept_sales = df_analysis.groupby('Dept')['Weekly_Sales'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=dept_sales.index.astype(str), y=dept_sales.values, palette='Blues_d')
plt.title('Top 10 Departments by Average Weekly Sales')
plt.xlabel('Department')
plt.ylabel('Average Weekly Sales')
plt.show()

print("Top 10 Departments by Average Weekly Sales:\n", dept_sales)

# Task 5 — Bivariate Analysis
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Use cleaned and outlier-treated dataset
df_bivar = df_clean.copy()

# ------------------------------
# 1️⃣ Temperature vs Weekly Sales (Scatter Plot)
plt.figure(figsize=(8,5))
sns.scatterplot(x='Temperature', y='Weekly_Sales', data=df_bivar, hue='IsHoliday', palette=['green','red'])
plt.title('Temperature vs Weekly Sales')
plt.xlabel('Temperature')
plt.ylabel('Weekly Sales')
plt.show()

# ------------------------------
# 2️⃣ Fuel Price vs Weekly Sales (Scatter Plot)
plt.figure(figsize=(8,5))
sns.scatterplot(x='Fuel_Price', y='Weekly_Sales', data=df_bivar, hue='IsHoliday', palette=['blue','orange'])
plt.title('Fuel Price vs Weekly Sales')
plt.xlabel('Fuel Price')
plt.ylabel('Weekly Sales')
plt.show()

# ------------------------------
# 3️⃣ Weekly Sales vs Store Type (Box Plot)
plt.figure(figsize=(8,5))
sns.boxplot(x='Type', y='Weekly_Sales', data=df_bivar, palette='Set2')
plt.title('Weekly Sales vs Store Type')
plt.xlabel('Store Type')
plt.ylabel('Weekly Sales')
plt.show()

# ------------------------------
# 4️⃣ Weekly Sales vs Holiday_Flag (Box Plot)
plt.figure(figsize=(6,5))
sns.boxplot(x='IsHoliday', y='Weekly_Sales', data=df_bivar, palette=['grey','red'])
plt.title('Weekly Sales vs Holiday Flag')
plt.xlabel('IsHoliday')
plt.ylabel('Weekly Sales')
plt.show()

# ------------------------------
# 5️⃣ Compare sales between top-performing and lowest-performing store
store_avg_sales = df_bivar.groupby('Store')['Weekly_Sales'].mean().sort_values(ascending=False)
top_store = store_avg_sales.index[0]
low_store = store_avg_sales.index[-1]

print(f"Top-performing Store: {top_store}, Average Weekly Sales: {store_avg_sales[top_store]:.2f}")
print(f"Lowest-performing Store: {low_store}, Average Weekly Sales: {store_avg_sales[low_store]:.2f}\n")

plt.figure(figsize=(10,5))
sns.lineplot(data=df_bivar[df_bivar['Store']==top_store].sort_values('Date'), x='Date', y='Weekly_Sales', label=f'Top Store {top_store}')
sns.lineplot(data=df_bivar[df_bivar['Store']==low_store].sort_values('Date'), x='Date', y='Weekly_Sales', label=f'Lowest Store {low_store}')
plt.title('Weekly Sales Comparison: Top vs Lowest Performing Store')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.legend()
plt.show()

# Task 6 — Multivariate Analysis
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Use cleaned and outlier-treated dataset
df_multi = df_clean.copy()

# ------------------------------
# 1️⃣ Correlation Heatmap for all numeric features
numeric_cols = df_multi.select_dtypes(include=['float64','int64']).columns

plt.figure(figsize=(10,8))
sns.heatmap(df_multi[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numeric Features')
plt.show()

# ------------------------------
# 2️⃣ Store-level sales analysis: Store Type vs Store Size vs Weekly Sales
plt.figure(figsize=(10,6))
sns.scatterplot(data=df_multi, x='Size', y='Weekly_Sales', hue='Type', palette='Set2', alpha=0.7)
plt.title('Weekly Sales vs Store Size by Store Type')
plt.xlabel('Store Size')
plt.ylabel('Weekly Sales')
plt.show()

# ------------------------------
# 3️⃣ Multivariate: Weekly Sales vs Temperature vs Holiday_Flag
plt.figure(figsize=(10,6))
sns.scatterplot(data=df_multi, x='Temperature', y='Weekly_Sales', hue='IsHoliday', palette=['green','red'], alpha=0.7)
plt.title('Weekly Sales vs Temperature (Holiday vs Non-Holiday)')
plt.xlabel('Temperature')
plt.ylabel('Weekly Sales')
plt.show()

# Grouped summary table
temp_holiday_summary = df_multi.groupby(['IsHoliday', pd.cut(df_multi['Temperature'], bins=5)])['Weekly_Sales'].mean().unstack()
print("Average Weekly Sales by Temperature bins and Holiday Flag:\n", temp_holiday_summary, "\n")

# ------------------------------
# 4️⃣ Analyze effect of markdowns on sales considering Date and Holidays
markdown_cols = [col for col in df_multi.columns if 'MarkDown' in col]

# Create a total markdown column
df_multi['Total_MarkDown'] = df_multi[markdown_cols].sum(axis=1)

plt.figure(figsize=(10,6))
sns.scatterplot(data=df_multi, x='Total_MarkDown', y='Weekly_Sales', hue='IsHoliday', palette=['blue','orange'], alpha=0.6)
plt.title('Weekly Sales vs Total Markdown (Holiday vs Non-Holiday)')
plt.xlabel('Total Markdown')
plt.ylabel('Weekly Sales')
plt.show()

# Optional: trend over time
markdown_time = df_multi.groupby('Date')[['Weekly_Sales','Total_MarkDown']].sum()
fig, ax1 = plt.subplots(figsize=(12,5))
ax1.plot(markdown_time.index, markdown_time['Weekly_Sales'], color='blue', label='Weekly Sales')
ax2 = ax1.twinx()
ax2.plot(markdown_time.index, markdown_time['Total_MarkDown'], color='red', label='Total MarkDown')
ax1.set_xlabel('Date')
ax1.set_ylabel('Weekly Sales', color='blue')
ax2.set_ylabel('Total MarkDown', color='red')
plt.title('Weekly Sales and Total MarkDown Over Time')
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')
plt.show()

# Task 7 — Time Series Analysis
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Use cleaned and outlier-treated dataset
df_ts = df_clean.copy()

# ------------------------------
# 1️⃣ Convert Date to Year, Month, Week
df_ts['Date'] = pd.to_datetime(df_ts['Date'], errors='coerce')
df_ts['Year'] = df_ts['Date'].dt.year
df_ts['Month'] = df_ts['Date'].dt.month
df_ts['Week'] = df_ts['Date'].dt.isocalendar().week

# ------------------------------
# 2️⃣ Plot total weekly sales over time (all stores)
weekly_sales_total = df_ts.groupby('Date')['Weekly_Sales'].sum()

plt.figure(figsize=(12,5))
plt.plot(weekly_sales_total.index, weekly_sales_total.values, color='blue')
plt.title('Total Weekly Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.show()

# ------------------------------
# 3️⃣ Monthly sales trends for top and lowest performing store
# Monthly sales trends for top and lowest performing store
plt.figure(figsize=(12,6))

# Plot top store
sns.lineplot(data=monthly_sales_top, x='Month', y='Weekly_Sales', hue='Year', marker='o', palette='Blues')
plt.title(f'Monthly Sales Trend - Top Store {top_store}')
plt.xlabel('Month')
plt.ylabel('Monthly Sales')
plt.show()

# Plot lowest store
plt.figure(figsize=(12,6))
sns.lineplot(data=monthly_sales_low, x='Month', y='Weekly_Sales', hue='Year', marker='o', palette='Reds')
plt.title(f'Monthly Sales Trend - Lowest Store {low_store}')
plt.xlabel('Month')
plt.ylabel('Monthly Sales')
plt.show()

# ------------------------------
# 4️⃣ Identify seasonal patterns

# a) Peak months across all stores
monthly_sales_total = df_ts.groupby('Month')['Weekly_Sales'].sum().sort_values(ascending=False)
print("Peak Sales Months:\n", monthly_sales_total.head(5), "\n")

# b) Departments with seasonal demand (sum of sales by month & dept)
dept_month_sales = df_ts.groupby(['Dept','Month'])['Weekly_Sales'].sum().reset_index()
# Top 5 departments with highest seasonal spikes
seasonal_depts = dept_month_sales.groupby('Dept')['Weekly_Sales'].max().sort_values(ascending=False).head(5)
print("Departments with Peak Seasonal Demand:\n", seasonal_depts)

# Optional: heatmap to visualize department seasonality
dept_month_pivot = dept_month_sales.pivot(index='Dept', columns='Month', values='Weekly_Sales')
plt.figure(figsize=(12,8))
sns.heatmap(dept_month_pivot, cmap='YlGnBu', linewidths=0.5)
plt.title('Department-wise Monthly Sales (Seasonality)')
plt.xlabel('Month')
plt.ylabel('Department')
plt.show()

# Task 8 — Feature Engineering
import pandas as pd

# Use cleaned and time-converted dataset
df_feat = df_clean.copy()

# ------------------------------
# 8A — Create New Columns

# Year, Month, Week already exist
df_feat['Year'] = df_feat['Date'].dt.year
df_feat['Month'] = df_feat['Date'].dt.month
df_feat['Week'] = df_feat['Date'].dt.isocalendar().week

# Discount effect
markdown_cols = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']
df_feat['discount_effect'] = df_feat[markdown_cols].sum(axis=1)

# Peak season flag (Nov, Dec)
df_feat['is_peak_season'] = df_feat['Month'].isin([11,12])

# Normalized sales
df_feat['normalized_sales'] = df_feat['Weekly_Sales'] / df_feat['Size']

# ------------------------------
# 8B — Filtering Using Created Columns

# Peak season transactions with weekly_sales > 50,000
peak_sales = df_feat[(df_feat['is_peak_season']==True) & (df_feat['Weekly_Sales']>50000)]
print("Peak-season transactions with Weekly Sales > 50,000:\n", peak_sales.head(), "\n")

# Stores with normalized_sales in top 10% percentile
top_norm_sales = df_feat[df_feat['normalized_sales'] >= df_feat['normalized_sales'].quantile(0.90)]
print("Stores with top 10% normalized sales:\n", top_norm_sales[['Store','normalized_sales']].drop_duplicates().head(), "\n")

# Departments where discount_effect > median
high_discount_depts = df_feat[df_feat['discount_effect'] > df_feat['discount_effect'].median()]
print("Departments with discount_effect above median:\n", high_discount_depts[['Dept','discount_effect']].drop_duplicates().head(), "\n")

# Rows meeting multiple filters
filtered_rows = df_feat[
    (df_feat['Temperature']<40) &
    (df_feat['Fuel_Price']>3.5) &
    (df_feat['Weekly_Sales'].between(20000,60000)) &
    (df_feat['is_peak_season']==True)
]
print("Filtered rows meeting multiple criteria:\n", filtered_rows.head(), "\n")

# ------------------------------
# 8C — Grouping & Business Insights

# Monthly average sales per store
monthly_avg_store = df_feat.groupby(['Store','Month'])['Weekly_Sales'].mean().reset_index()
print("Monthly average sales per store:\n", monthly_avg_store.head(), "\n")

# Total discount_effect per department
total_discount_dept = df_feat.groupby('Dept')['discount_effect'].sum().sort_values(ascending=False)
print("Total discount_effect per department:\n", total_discount_dept.head(), "\n")

# Department with highest normalized_sales
dept_norm_sales = df_feat.groupby('Dept')['normalized_sales'].mean().sort_values(ascending=False)
top_dept_norm = dept_norm_sales.idxmax()
print(f"Department with highest normalized sales: {top_dept_norm}, Value: {dept_norm_sales.max():.2f}\n")

# Store-wise revenue potential (Weekly_Sales * 52)
store_revenue = df_feat.groupby('Store')['Weekly_Sales'].sum() * 52 / df_feat.groupby('Store')['Weekly_Sales'].count()
store_revenue = store_revenue.sort_values(ascending=False)
print("Top 10 stores by revenue potential:\n", store_revenue.head(10))