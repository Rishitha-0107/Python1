# -*- coding: utf-8 -*-
"""Day-19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1skyA_5-IaUIduio2YWjd4g0HKDH89oox

**1Ô∏è Load the Dataset & Identify Relevant Columns**
"""

import pandas as pd
df = pd.read_csv("CarPrice_Assignment.csv")

df.head()
df.info()

df.isnull().sum()

df.isnull().sum().sum()

df = df.drop_duplicates()

df.isnull().sum()
df.shape

"""**2. Define Input and Output Variables**"""

X = df[['enginesize']]
y = df['price']

"""**3. Visualize the relationship between engine size and price.**"""

import matplotlib.pyplot as plt

plt.scatter(X, y)
plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Engine Size vs Car Price")
plt.show()

"""**4Ô∏è)Build a Predictive Model (Linear Regression)**"""

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)

plt.scatter(X, y)
plt.plot(X, lin_reg.predict(X))
plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Linear Regression Model")
plt.show()

"""5. Predict the price for a car with:
 Engine size = 200 **bold text**
"""

lin_pred = lin_reg.predict([[200]])
lin_pred

"""**6Ô∏è‚É£ Increase Model Flexibility (Polynomial Regression)**"""

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=3)   # cubic curve
X_poly = poly.fit_transform(X)

poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)

# Sort values for smooth curve
X_sorted, y_sorted = zip(*sorted(zip(X.values.flatten(), y)))

X_sorted = pd.DataFrame(X_sorted, columns=['enginesize'])
X_sorted_poly = poly.transform(X_sorted)

plt.scatter(X, y)
plt.plot(X_sorted, poly_reg.predict(X_sorted_poly))
plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Polynomial Regression Model (Degree 3)")
plt.show()

"""**7Ô∏è‚É£ Predict Price for Engine Size = 200 (Polynomial Model)**"""

poly_pred = poly_reg.predict(poly.transform([[200]]))
poly_pred

"""8. Analyze which model:
captures the trend better
gives more realistic predictions **bold text**

üîπ Which model captures the trend better?
Model	Trend Capture
Linear Regression	‚ùå Poor
Polynomial Regression	‚úÖ Good

Reason:
The data shows:

Slow price increase for small engines

Rapid increase for medium engines

Saturation for large engines

A straight line cannot bend, while a polynomial curve adapts to this behavior.

‚úî Polynomial regression captures the trend better

üîπ Which model gives more realistic predictions?
Model	Prediction Realism
Linear Regression	‚ùå Unrealistic
Polynomial Regression	‚úÖ Realistic

Why?

Linear model keeps increasing price endlessly

Polynomial model reflects market saturation in luxury cars

‚úî Curved model gives realistic pricing estimates
"""

poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)

poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)

# Sort data for smooth curve
X_sorted = X.sort_values(by='enginesize')
X_sorted_poly = poly.transform(X_sorted)

plt.figure(figsize=(8,6))

# Actual data points
plt.scatter(X, y, label="Actual Data")

# Linear model
plt.plot(X_sorted, lin_reg.predict(X_sorted), label="Linear Model")

# Polynomial model
plt.plot(X_sorted, poly_reg.predict(X_sorted_poly), label="Polynomial Model (Degree 3)")

plt.xlabel("Engine Size")
plt.ylabel("Car Price")
plt.title("Linear vs Curved Model: Engine Size vs Price")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Example dataset: Study Hours vs. Pass/Fail (Binary Classification)
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]) # Study Hours
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) # 0 = Fail, 1 = Pass

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Predictions:", y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Generate values for plotting
X_range = np.linspace(0, 12, 100).reshape(-1, 1)
y_prob = model.predict_proba(X_range)[:, 1] # Get probability of class 1

# Plot
plt.scatter(X, y, color="red", label="Actual Data")
plt.plot(X_range, y_prob, color="blue", label="Logistic Regression Curve")
plt.xlabel("Study Hours")
plt.ylabel("Probability of Passing")
plt.title("Logistic Regression - Study Hours vs. Pass/Fail")
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
ConfusionMatrixDisplay.from_predictions(
y_test,
y_pred,
cmap="Blues",
values_format="d"
)
plt.title("Confusion Matrix")
plt.show()

import pandas as pd

df = pd.read_csv("heart.csv")
df.head()
df.info()
df.isnull().sum()
df.duplicated().sum()
df = df.drop_duplicates()
X = df.drop('target', axis=1)
y = df['target']

num_features = [
    'age', 'trestbps', 'chol',
    'thalach', 'oldpeak', 'ca'
]
cat_features = [
    'sex', 'cp', 'fbs', 'restecg',
    'exang', 'slope', 'thal'
]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X[num_features] = scaler.fit_transform(X[num_features])
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

y_pred = log_reg.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
accuracy

cm = confusion_matrix(y_test, y_pred)
cm

print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_curve, auc

# Get predicted probabilities
y_prob = log_reg.predict_proba(X_test)[:, 1]

# ROC values
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Create confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,5))

# Use a light color map
plt.imshow(cm, cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.colorbar()

# Add values inside matrix
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(
            j, i, cm[i, j],
            ha="center", va="center",
            color="black"
        )

plt.tight_layout()
plt.show()

