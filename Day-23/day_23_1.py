# -*- coding: utf-8 -*-
"""Day-23_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MF3Eil65Kg0VPG48lcxEZZdcZWZvLbzE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#getting dataset from sklearn
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

cancer['feature_names']

cancer['data']

dataset = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns=np.append(cancer['feature_names'] , ['target']))
dataset.head()

#checking empty values if any
dataset.isna().sum()

#creating feature matrix and dependent variable vector
X = dataset.iloc[: , :- 1].values
y = dataset.iloc[: , -1].values

#training set and test set
from sklearn.model_selection import train_test_split
x_train , x_test , y_train ,y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test =sc.transform(x_test)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=25 ,metric = 'minkowski')
classifier.fit(x_train , y_train)

y_pred = classifier.predict(x_test)
y_pred

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy Score: {accuracy:.4f}")

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

import pandas as pd

df = pd.read_csv('credit_risk_dataset.csv')
display(df.head())

print('DataFrame Info:')
df.info()

print('\nDescriptive Statistics for Numerical Columns:')
display(df.describe())

print('Unique values for person_home_ownership:')
print(df['person_home_ownership'].value_counts())
print('\nUnique values for loan_intent:')
print(df['loan_intent'].value_counts())
print('\nUnique values for loan_grade:')
print(df['loan_grade'].value_counts())
print('\nUnique values for cb_person_default_on_file:')
print(df['cb_person_default_on_file'].value_counts())

print(f"Data type of 'loan_status': {df['loan_status'].dtype}")
print(f"Unique values in 'loan_status': {df['loan_status'].unique()}")

print("Missing values before imputation:")
print(df[['person_emp_length', 'loan_int_rate']].isnull().sum())

median_emp_length = df['person_emp_length'].median()
median_loan_int_rate = df['loan_int_rate'].median()

df['person_emp_length'].fillna(median_emp_length, inplace=True)
df['loan_int_rate'].fillna(median_loan_int_rate, inplace=True)

print(f"Median for person_emp_length: {median_emp_length}")
print(f"Median for loan_int_rate: {median_loan_int_rate}")

median_emp_length = df['person_emp_length'].median()
median_loan_int_rate = df['loan_int_rate'].median()

df['person_emp_length'] = df['person_emp_length'].fillna(median_emp_length)
df['loan_int_rate'] = df['loan_int_rate'].fillna(median_loan_int_rate)

print(f"Median for person_emp_length: {median_emp_length}")
print(f"Median for loan_int_rate: {median_loan_int_rate}")

print("Missing values after imputation:")
print(df[['person_emp_length', 'loan_int_rate']].isnull().sum())

categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
display(df_encoded.head())
print(f"Original DataFrame shape: {df.shape}")
print(f"Encoded DataFrame shape: {df_encoded.shape}")

X = df_encoded.drop('loan_status', axis=1)
y = df_encoded['loan_status']

print("Features (X) shape:", X.shape)
print("Target (y) shape:", y.shape)

numerical_cols = X.select_dtypes(include=np.number).columns
print(f"Numerical columns identified for scaling: {list(numerical_cols)}")

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

print("Numerical features scaled successfully.")
display(X.head())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

print("Initial KNN model trained successfully.")

y_pred_initial = knn.predict(X_test)
print("Predictions for the initial KNN model (n_neighbors=5) generated successfully.")

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

accuracy_initial = accuracy_score(y_test, y_pred_initial)
cm_initial = confusion_matrix(y_test, y_pred_initial)

print(f"Initial KNN Model (n_neighbors=5) Accuracy: {accuracy_initial:.4f}")
print("\nInitial KNN Model Confusion Matrix:")
print(cm_initial)
print("\nInitial KNN Model Classification Report:")
print(classification_report(y_test, y_pred_initial))

accuracy_scores = []
k_values = range(1, 40)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy_scores.append(accuracy_score(y_test, y_pred))

print("Accuracy scores for different k values calculated.")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracy_scores, marker='o', linestyle='-')
plt.title('KNN Accuracy vs. Number of Neighbors (k)')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy Score')
plt.xticks(k_values[::2])
plt.grid(True)
plt.show()

print("Accuracy plot generated successfully.")

optimal_k_index = accuracy_scores.index(max(accuracy_scores))
optimal_k = k_values[optimal_k_index]
optimal_accuracy = accuracy_scores[optimal_k_index]

print(f"The optimal number of neighbors (k) is: {optimal_k}")
print(f"The highest accuracy achieved is: {optimal_accuracy:.4f}")

"""When K is very small (e.g., K=1):

High Variance, Low Bias: A very small K, such as K=1 (1-NN), makes the model highly sensitive to noise in the training data. The decision boundary becomes very complex and erratic, fitting the training data very closely.
Overfitting: This often leads to overfitting, where the model performs exceptionally well on the training data but poorly on unseen test data because it has learned the noise rather than the underlying patterns.
Sensitivity to Outliers: Outliers can significantly influence the classification of a new data point, as just one noisy neighbor can determine the class.
Looking at our accuracy curve, we can see that for K=1, the accuracy is around 0.857, which is lower than the optimal K=7.
When K is very large (e.g., K approaching the size of the dataset):

Low Variance, High Bias: A very large K makes the model less sensitive to individual data points and noise. The decision boundary becomes smoother and simpler, as it considers a larger neighborhood.
Underfitting: If K is too large, the model might include neighbors from other classes, leading to a decision boundary that is too general and doesn't capture the true patterns in the data. This results in underfitting, where the model performs poorly on both training and test data.
Loss of Locality: The advantage of locality (that nearby points are similar) is lost when considering a very large neighborhood, as the classification of a point might be influenced by data points that are far away.
Our plot shows that as K increases beyond the optimal value, the accuracy generally starts to decrease, illustrating this effect.
In our case, the optimal K value of 7 represents a good balance, avoiding the pitfalls of both very small and very large K values, yielding the highest accuracy of 0.8924.

"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(cm_initial, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Initial KNN Model, n_neighbors=5)')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

model = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt']
}

grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

model = RandomForestClassifier(random_state=42)

random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=30,   # ðŸ”¥ key
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

random_search.fit(X_train, y_train)

print("Best Parameters:", random_search.best_params_)
print("Best Score:", random_search.best_score_)

dataset=pd.read_csv('Social_Network_Ads.csv')
dataset.head()

X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.2, random_state=42

)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

base_model = DecisionTreeClassifier(max_depth=1)

classifier = AdaBoostClassifier(
    estimator=base_model,
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)

# Train model
classifier.fit(X_train, y_train)

# -----------------------------
# Predictions
# -----------------------------
y_pred = classifier.predict(X_test)

# -----------------------------
# Accuracy
# -----------------------------
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# -----------------------------
# Confusion Matrix
# -----------------------------
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# -----------------------------
# Plot Confusion Matrix
# -----------------------------
plt.figure(figsize=(6, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Not Purchased", "Purchased"],
    yticklabels=["Not Purchased", "Purchased"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - AdaBoost Classifier")
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# Load Dataset
df = pd.read_csv("Social_Network_Ads.csv")

# Feature Selection
X = df.iloc[:, [2, 3]].values
y = df.iloc[:, -1].values

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Gradient Boosting Model
classifier = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

# Train model
classifier.fit(X_train, y_train)

# -----------------------------
# Predictions
# -----------------------------
y_pred = classifier.predict(X_test)

# -----------------------------
# Accuracy
# -----------------------------
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# -----------------------------
# Confusion Matrix
# -----------------------------
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# -----------------------------
# Plot Confusion Matrix
# -----------------------------
plt.figure(figsize=(6, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Reds",
    xticklabels=["Not Purchased", "Purchased"],
    yticklabels=["Not Purchased", "Purchased"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Gradient Boosting Classifier")
plt.show()

# Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix

from xgboost import XGBClassifier
# Load Dataset
df = pd.read_csv("Social_Network_Ads.csv")
# Feature Selection
# Using Age and EstimatedSalary
X = df.iloc[:, [2, 3]].values
y = df.iloc[:, -1].values
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
# XGBoost Model
classifier = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Train model
classifier.fit(X_train, y_train)
# Predictions
y_pred = classifier.predict(X_test)
# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
# Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Not Purchased", "Purchased"],
    yticklabels=["Not Purchased", "Purchased"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - XGBoost Classifier")
plt.show()

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("Telco-Customer-Churn.csv")

# Preview
df.head()

# Select important attributes
df[['tenure', 'MonthlyCharges', 'Contract', 'PaymentMethod', 'Churn']].head()

# Dataset info
df.info()

# Churn distribution
df['Churn'].value_counts()

# Encode target variable
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Convert TotalCharges to numeric
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Fill missing values
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Drop customerID
df.drop('customerID', axis=1, inplace=True)

# One-hot encoding
df_encoded = pd.get_dummies(df, drop_first=True)

df_encoded.head()

X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

print(X.shape, y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, recall_score

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

y_pred_dt = dt.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Recall (Churn):", recall_score(y_test, y_pred_dt))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))

import seaborn as sns
import matplotlib.pyplot as plt

cm_dt = confusion_matrix(y_test, y_pred_dt)

sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Decision Tree Confusion Matrix")
plt.show()

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import precision_score, f1_score

gb = GradientBoostingClassifier(
    n_estimators=150,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

gb.fit(X_train, y_train)

y_pred_gb = gb.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred_gb))
print("Precision:", precision_score(y_test, y_pred_gb))
print("Recall:", recall_score(y_test, y_pred_gb))
print("F1 Score:", f1_score(y_test, y_pred_gb))

comparison = pd.DataFrame({
    'Model': ['Decision Tree', 'Gradient Boosting'],
    'Accuracy': [
        accuracy_score(y_test, y_pred_dt),
        accuracy_score(y_test, y_pred_gb)
    ],
    'Recall (Churn)': [
        recall_score(y_test, y_pred_dt),
        recall_score(y_test, y_pred_gb)
    ]
})

comparison

cm_gb = confusion_matrix(y_test, y_pred_gb)

TN, FP, FN, TP = cm_gb.ravel()

print("Correctly identified churn customers (TP):", TP)
print("Loyal customers wrongly flagged (FP):", FP)
print("Missed churn customers (FN):", FN)

feature_importance = pd.Series(
    gb.feature_importances_,
    index=X.columns
).sort_values(ascending=False)

feature_importance.head(10)

feature_importance.head(10).plot(kind='barh', figsize=(8,5))
plt.title("Top Features Influencing Churn")
plt.show()