# -*- coding: utf-8 -*-
"""Day-24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cp5vrDPz2Zjnue9oe5fvGg0Nnjparxqh

**STACKING**
"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

dataset=pd.read_csv('Social_Network_Ads.csv')
dataset.head()

dataset.isnull().sum()

X = dataset.iloc[:, [2, 3]].values
# Age, EstimatedSalary

y = dataset.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42

)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""**DEFINE BASE MODELS**"""

base_models = [
    ('lr', LogisticRegression()),
     ('dt', DecisionTreeClassifier(max_depth=3)),
      ('knn', KNeighborsClassifier(n_neighbors=5))
]

"""**DEFINE META MODEL**"""

meta_model=LogisticRegression()

"""**BUILD STACKING CLASSIFIER**"""

classifier = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5

)

classifier.fit(X_train, y_train)

accuracy_score(y_test, classifier.predict(X_test))

confusion_matrix=confusion_matrix(y_test, classifier.predict(X_test))
confusion_matrix

"""**TASK-1**"""

import pandas as pd

# Load dataset
df = pd.read_csv('kc_house_data.csv')

# Display basic info
df.head()

target = 'price'

numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_features.remove(target)
numerical_features

categorical_features = df.select_dtypes(include=['object']).columns.tolist()
categorical_features

# Check missing values
df.isnull().sum()

df = pd.get_dummies(df, columns=categorical_features, drop_first=True)

from sklearn.model_selection import train_test_split

X = df.drop(target, axis=1)
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""**Task 2: Build Base Models (Regression)**"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score

models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "KNN": KNeighborsRegressor(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "SVM": SVR()
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    score = r2_score(y_test, preds)
    results[name] = score
    print(f"{name} RÂ² Score: {score:.4f}")

best_model = max(results, key=results.get)
best_model, results[best_model]

"""**Task 3: Implement Stacking Model (CORE TASK)**"""

from sklearn.model_selection import KFold
import numpy as np

kf = KFold(n_splits=5, shuffle=True, random_state=42)

train_meta = np.zeros((X_train.shape[0], len(models)))
test_meta = np.zeros((X_test.shape[0], len(models)))

"""**K-MEANS CLUSTERING**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dataset = pd.read_csv('Mall_Customers.csv')
dataset.head()

dataset.isna().sum()

df_customers = pd.read_csv('Mall_Customers.csv')
print("First 5 rows of the dataset:")
print(df_customers.head())
print("\nMissing values in the dataset:")
print(df_customers.isnull().sum())

numerical_features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
categorical_features = ['Genre']
df_processed = df_customers.drop('CustomerID', axis=1)
df_processed = pd.get_dummies(df_processed, columns=categorical_features, drop_first=True)
print("DataFrame after dropping CustomerID and one-hot encoding 'Genre':")
print(df_processed.head())

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_processed[numerical_features] = scaler.fit_transform(df_processed[numerical_features])

print("DataFrame after standardizing numerical features:")
print(df_processed.head())

from sklearn.cluster import KMeans

inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(df_processed)
    inertia.append(kmeans.inertia_)
print("Inertia values calculated for k from 1 to 10.")

plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), inertia, marker='o', linestyle='--')
plt.title('Elbow Method For Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.xticks(range(1, 11))
plt.show()
print("Elbow curve plotted successfully.")

kmeans = KMeans(n_clusters=5 , init='k-means++' , random_state=0)
y_kmeans = kmeans.fit_predict(X)

y_kmeans

kmeans = KMeans(n_clusters=5 , init='k-means++' , random_state=0, n_init=10)
y_kmeans = kmeans.fit_predict(df_processed)

kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0)
y_kmeans=kmeans.fit_predict(X)