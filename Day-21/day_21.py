# -*- coding: utf-8 -*-
"""Day-21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ct7CLQNuXzVfqoudhtbwserNlQfR_6jh
"""

import numpy as np
import matplotlib.pyplot as plt

# Units consumed
X = np.array([50, 120, 180, 250, 320, 380]).reshape(-1, 1)

# Corresponding electricity bill
y = np.array([500, 1000, 1000, 1800, 2500, 2500])

from sklearn.tree import DecisionTreeRegressor
dt_reg=DecisionTreeRegressor(max_depth=3,random_state=42)
dt_reg.fit(X, y)

X_grid = np.linspace(0, 400, 200).reshape(-1, 1)
y_pred = dt_reg.predict(X_grid)

plt.scatter(X, y, color='blue')
plt.plot(X_grid, y_pred, color='red')
plt.xlabel("Units Consumed")
plt.ylabel("Electricity Bill (₹)")
plt.title("Decision Tree Regression - Electricity Bill Slabs")
plt.show()

units = float(input("Enter electricity units consumed: "))
bill = dt_reg.predict([[units]])

print(f"Predicted Electricity Bill: ₹{bill[0]:.0f}")

"""Predict salary based on years of experience using Decision Tree Regression

Input ->Years of Experience

Output -> Salary

"""

import numpy as np
import matplotlib.pyplot as plt

X = np.array([1, 2, 3, 4, 5, 7, 8, 9]).reshape(-1, 1)
y = np.array([3, 3.5, 4, 4.2, 6, 6.5, 7, 7.2])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=42

)

from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor(random_state=42)
dt_reg.fit(X_train, y_train)

X_grid = np.linspace(1, 8, 100).reshape(-1, 1)
y_pred = dt_reg.predict(X_grid)

plt.scatter(X, y, color='blue')
plt.plot(X_grid, y_pred, color='red' )
plt.title("Decision Tree Regression")
plt.xlabel("Years of Experience")
plt.ylabel ("Salary (|P4)")
plt.show()

dt_controlled = DecisionTreeRegressor(max_depth=3)
dt_controlled.fit(X_train, y_train)

y_pred_controlled = dt_controlled.predict(X_grid)

plt.scatter(X, y)
plt.plot(X_grid, y_pred_controlled, color='orange')
plt.title("Controlled Tree (max_depth=3)")
plt.show()

from sklearn.metrics import mean_squared_error, r2_score

# Predictions
y_test_pred_deep = dt_deep.predict(X_test)
y_test_pred_controlled = dt_controlled.predict(X_test)

# Metrics
print("Deep Tree Metrics")
print("MSE:", mean_squared_error(y_test, y_test_pred_deep))
print("R2 :", r2_score(y_test, y_test_pred_deep))

print("\nControlled Tree Metrics")
print("MSE:", mean_squared_error(y_test, y_test_pred_controlled) )
print("R2 :", r2_score(y_test, y_test_pred_controlled) )

dt_deep = DecisionTreeRegressor(max_depth=None)
dt_deep.fit(X_train, y_train)
y_pred_deep = dt_deep.predict(X_grid)

plt.scatter(X, y)
plt.plot(X_grid, y_pred_deep, color='green')
plt.title("Overfittling Tree")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

np.random.seed(42)

X = np.arange(1, 21).reshape(-1, 1)
y = np.array([
    10, 12, 15, 18, 20, 25, 30, 28, 35, 40,
    45, 42, 50, 55, 60, 58, 65, 70, 72, 75
])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

dt_reg = DecisionTreeRegressor(max_depth=4, random_state=42)
dt = DecisionTreeRegressor(random_state=42)
dt.fit(X_train, y_train)

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(
n_estimators=100,
random_state=42
)
rf.fit(X_train, y_train)

X_grid = np.linspace(1, 20, 200).reshape(-1, 1)

plt.scatter(X, y, color='black')

plt.plot(X_grid, dt.predict(X_grid), color='red', label='Decision Tree')
plt.plot(X_grid, rf.predict(X_grid), color='green', label='Random Forest')

plt.xlabel("House Size Index")
plt.ylabel("Price")
plt.title("Decision Tree vs Random Forest Regression")
plt.legend()
plt.show()

print("Decision Tree Regression Metrics")
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_test_pred))
print("R2 Score:", r2_score(y_test, y_test_pred))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

import pandas as pd

df = pd.read_csv('house_data.csv')

df.head()

df.info()

df.describe()

df.isnull().sum()

X = df[
    [
        'sqft_living',
        'bedrooms',
        'bathrooms',
        'floors',
        'waterfront',
        'view',
        'condition',
        'sqft_above',
        'sqft_basement',
        'yr_built'
    ]
]

y = df['price']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

dt = DecisionTreeRegressor(
    max_depth=6,
    random_state=42
)

dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

rf = RandomForestRegressor(
    n_estimators=200,
    random_state=42
)

rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Decision Tree Performance")
print("MSE:", mean_squared_error(y_test, y_pred_dt))
print("R2 :", r2_score(y_test, y_pred_dt))

print("\nRandom Forest Performance")
print("MSE:", mean_squared_error(y_test, y_pred_rf))
print("R2 :", r2_score(y_test, y_pred_rf))

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred_dt)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Decision Tree: Actual vs Predicted")
plt.grid(True)
plt.show()

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred_rf)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Random Forest: Actual vs Predicted")
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
# Study hours (X) and Marks (y)
X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([35, 40, 50, 60, 65, 70])

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=42

)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

y_train_pred = lin_reg.predict(X_train)
y_test_pred = lin_reg.predict(X_test)

print("GOOD FIT (Linear Regression)")
print("Train MSE:", mean_squared_error(y_train, y_train_pred) )
print("Train R2 :", r2_score(y_train, y_train_pred))

plt.scatter(X, y)
plt.plot(X, lin_reg.predict(X), color='green')
plt.title("Good Fit: Marks vs Study Hours")
plt.xlabel("Study Hours")
plt.ylabel("Marks")
plt.show()

poly = PolynomialFeatures(degree=5)

X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly. transform(X_test)

overfit_model = LinearRegression()
overfit_model.fit(X_train_poly, y_train)

y_train_pred_poly = overfit_model.predict(X_train_poly)
y_test_pred_poly = overfit_model.predict(X_test_poly)

print("\nOVERFITTING MODEL (Polynomial Degree = 5)")
print("Train MSE:", mean_squared_error(y_train, y_train_pred_poly))
print("Train R2 :", r2_score(y_train, y_train_pred_poly))

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=5)
X_poly = poly.fit_transform(X)

from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1)
ridge.fit(X_poly, y)

y_ridge_pred = ridge.predict(X_poly)

from sklearn.metrics import mean_squared_error, r2_score

print("Ridge MSE:", mean_squared_error(y, y_ridge_pred))
print("Ridge R2 :", r2_score(y, y_ridge_pred))

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1) # You can adjust alpha as needed
lasso.fit(X_poly, y)
y_lasso_pred = lasso.predict(X_poly)

print("Ridge MSE:", mean_squared_error(y, y_ridge_pred))
print("Ridge R2 :", r2_score(y, y_ridge_pred))

print("\nLasso MSE:", mean_squared_error(y, y_lasso_pred) )
print("Lasso R2 :", r2_score(y, y_lasso_pred))

import pandas as pd

# Load the uploaded CSV file
df = pd.read_csv('spam.csv', encoding='latin-1')

# Select required columns
df = df[['v1', 'v2']]

# Rename columns
df.columns = ['label', 'text']

# Convert labels to numeric
df['label'] = df['label'].map({'ham': 0, 'spam': 1})

# Display first rows
print(df.head())

"""**Gaussian Naive Bayes (For Continuou's Data)**"""

df['text_length' ] = df['text' ].apply(len)
df['num_words' ] = df['text'].apply(lambda x: len(x.split()))
df['num_digits'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))

# Define features and target
X = df[['text_length', 'num_words', 'num_digits' ]]
y = df['label']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.naive_bayes import GaussianNB

# Train Gaussian Naive Bayes
gnb = GaussianNB()
gnb.fit(X_train_scaled, y_train)
y_pred_gnb = gnb.predict(X_test_scaled)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Accuracy
accuracy = accuracy_score(y_test, y_pred_gnb)
print("Gaussian Naive Bayes Accuracy:", accuracy)

"""**Multinomial Naive Bayes (For Text Data)**"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

# Convert text data into word count vectors
vectorizer = CountVectorizer(stop_words='english')
X_counts = vectorizer.fit_transform(df['text'])

# Convert counts to TF-IDF representation
tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_counts)
# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)

# Train Multinomial Naïve Bayes
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_pred_mnb = mnb.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_mnb)
print("Multinomial Naive Bayes Accuracy:", accuracy)

cm = confusion_matrix(y_test, y_pred_mnb)
print("Confusion Matrix:\n", cm)

print("Classification Report:\n")
print(classification_report(y_test, y_pred_mnb))

"""**Bernoulli Naive Bayes (For Binary Features)**"""

from sklearn.naive_bayes import BernoulliNB

# Convert text into binary presence/absence of important words
important_words = ["free", "win", "offer", "money", "urgent"]

for word in important_words:
    df[word] = df['text'].apply(
        lambda x: 1 if word in x.lower() else 0
    )
# Define features and target
X = df[important_words]
y = df ['label' ]
# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train Bernoulli Naive Bayes
bnb = BernoulliNB()
bnb.fit(X_train, y_train)
y_pred_bnb = bnb.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

# Accuracy
accuracy = accuracy_score(y_test, y_pred_bnb)
print("Bernoulli Naive Bayes Accuracy:", accuracy)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Create confusion matrices
cm_gnb = confusion_matrix(y_test, y_pred_gnb)
cm_mnb = confusion_matrix(y_test, y_pred_mnb)
cm_bnb = confusion_matrix(y_test, y_pred_bnb)

# Side-by-side plots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

ConfusionMatrixDisplay(cm_gnb, display_labels=["Ham", "Spam"]).plot(
    ax=axes[0], cmap="Blues", colorbar=False
)
axes[0].set_title("Gaussian Naive Bayes")

ConfusionMatrixDisplay(cm_mnb, display_labels=["Ham", "Spam"]).plot(
    ax=axes[1], cmap="Blues", colorbar=False
)
axes[1].set_title("Multinomial Naive Bayes")

ConfusionMatrixDisplay(cm_bnb, display_labels=["Ham", "Spam"]).plot(
    ax=axes[2], cmap="Blues", colorbar=False
)
axes[2].set_title("Bernoulli Naive Bayes")

plt.tight_layout()
plt.show()

"""**1️⃣ Load the dataset & explore messages and labels**"""

import pandas as pd

# Load dataset (uploaded directly in Colab)
df = pd.read_csv("spam.csv", encoding="latin-1")[["v1", "v2"]]
df.columns = ["label", "text"]

# Convert labels to binary
df["label"] = df["label"].map({"ham": 0, "spam": 1})

df.head()

"""**2️⃣ Count how many messages belong to each category**"""

df["label"].value_counts()

"""**Build a probabilistic classification model that can learn patterns from word occurrences.**"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(stop_words="english")
X = vectorizer.fit_transform(df["text"])
y = df["label"]

"""**Train the model using historical message data.**"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = MultinomialNB()
model.fit(X_train, y_train)

"""Predict whether unseen messages are:

Suspicious

Not suspicious
"""

y_pred = model.predict(X_test)

"""**Test the system using custom user-entered messages.**"""

custom_messages = [
    "Congratulations! You have won a free lottery ticket",
    "Can we meet tomorrow to discuss the project?",
    "Urgent! Claim your prize money now"
]

custom_vec = vectorizer.transform(custom_messages)
predictions = model.predict(custom_vec)

for msg, pred in zip(custom_messages, predictions):
    print(f"Message: {msg}")
    print("Prediction:", "Suspicious" if pred == 1 else "Not Suspicious")
    print()

"""**Evaluate the model using appropriate classification metrics.**"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=["Not Suspicious", "Suspicious"]))

"""**Analysis**"""

cm = confusion_matrix(y_test, y_pred)
cm

tn, fp, fn, tp = cm.ravel()

print("Suspicious messages correctly identified (TP):", tp)
print("Genuine messages incorrectly flagged (FP):", fp)