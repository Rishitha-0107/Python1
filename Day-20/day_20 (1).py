# -*- coding: utf-8 -*-
"""Day-20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14iOqtMUeD0UN-87NvSICDqn48niXYdGd
"""

import numpy as np
import pandas as pd
import seaborn as sns

# Load dataset
df = pd.read_csv("Social_Network_Ads.csv")

# View first few rows
df.head()

# Dataset shape
print("Shape:", df.shape)

# Column info
df.info()

# Check missing values
df.isnull().sum()

x=df.iloc[: ,2:4].values
y=df.iloc[:,-1].values

from sklearn.model_selection import train_test_split

# Split data (80% training, 20% testing)
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(
    criterion="entropy"

)
classifier.fit(x_train , y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

TN, FP, FN, TP = cm.ravel()

print("True Positives (TP):", TP)
print("True Negatives (TN):", TN)
print("False Positives (FP):", FP)
print("False Negatives (FN):", FN)

cm = confusion_matrix(y_test , y_pred)
sns.heatmap(cm , annot=True)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(18, 10))
plot_tree(
classifier,
feature_names=["Age", "Estimated Salary"],
class_names=["Not Purchased", "Purchased"],
filled=True,
rounded=True

)
plt.show()

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(
    criterion="entropy",
    max_depth=4
)
classifier.fit(x_train , y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

cm = confusion_matrix(y_test , y_pred)
sns.heatmap(cm , annot=True)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(18, 10))
plot_tree(
classifier,
feature_names=["Age", "Estimated Salary"],
class_names=["Not Purchased", "Purchased"],
filled=True,
rounded=True

)
plt.show()

"""**Decision Tree Classifier - Titanic Dataset**

Using Titanic passenger data, our goal is
to predict whether a passenger survived
or not based on their details.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import plot_tree

df = sns.load_dataset("titanic")

df.head()

df = df[["survived", "pclass", "sex", "age", "fare", "embarked"]]
df.head()

# Dataset shape
df.shape

# Column information
df.info()

# Check missing values
df.isna().sum()

df.isnull().values.any()

df["age"].fillna(df["age"].median(), inplace=True)
df["embarked"].fillna(df["embarked"].mode()[0], inplace=True)

encoder = LabelEncoder()

df["sex"] = encoder.fit_transform(df["sex"])
df["embarked"] = encoder.fit_transform(df["embarked"])

df.head()

X = df.drop("survived", axis=1)
y = df["survived"]

X.head(), y.head()

model = DecisionTreeClassifier(
    criterion="gini",
    max_depth=4,
    random_state=42
)
model.fit(X, y)

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(20, 10))
plot_tree(
    model,
    feature_names=X.columns,      # automatically uses feature names
    class_names=["Not Survived", "Survived"],
    filled=True

)
plt.title("Decision Tree (Gini Index, max_depth=4)")
plt.show()

new_passenger = pd.DataFrame([[3, 1, 25, 7, 2]], columns=X.columns)
prediction = model.predict(new_passenger)
prediction

if prediction[0] == 1:
  print("Passenger is likely to Survive")
else:
  print("Passenger is likely NOT to Survive")

"""**1. Load the dataset and understand customer attributes such as: Age Job Balance Loan status Contact history**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import numpy as np

"""**2. Identify input features that nhay influence whether a customer subscribes to a term deposit.**"""

df = pd.read_csv("bank_marketing_dataset.csv")

# Features and target
X = df.drop("deposit", axis=1)
y = df["deposit"].map({"no": 0, "yes": 1})

print(X.shape, y.shape)

"""**3. Build a model that can classify customers into: Likely to subscribe o Not likely to subscribe**"""

cat_cols = X.select_dtypes(include=['object']).columns
num_cols = X.select_dtypes(exclude=['object']).columns

preprocessor = ColumnTransformer([
    ("num", "passthrough", num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

"""**4. Train the model using historical customer data.**"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""**5️** **Predict for Unseen Customer Records**"""

rf_model = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(
        n_estimators=200,
        criterion="gini",        # Gini index
        max_depth=5,
        class_weight='balanced', # handle class imbalance
        random_state=42
    ))
])

"""**6️ Evaluate Model Performance**"""

rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)



# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# TP, TN, FP, FN
TN, FP, FN, TP = cm.ravel()
print("True Positives (TP):", TP)
print("True Negatives (TN):", TN)
print("False Positives (FP):", FP)
print("False Negatives (FN):", FN)

# Classification report
print(classification_report(y_test, y_pred, target_names=["Not Likely", "Likely"]))

"""**7️⃣ Visualize the Decision-Making Logic**"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
# Access the first tree in the Random Forest
single_tree = rf_model.named_steps["classifier"].estimators_[0]
# Get feature names after preprocessing
feature_names = list(num_cols) + \
                list(rf_model.named_steps["preprocessor"]
                     .named_transformers_["cat"]
                     .get_feature_names_out(cat_cols))
plt.figure(figsize=(25,15))
plot_tree(
    single_tree,
    feature_names=feature_names,
    class_names=["Not Likely", "Likely"],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Random Forest Decision Tree (Gini Index)")
plt.show()

df_cleaned = df[(df['age'] >= 40) & (df['age'] <= 60)]
print("Original shape:", df.shape)
print("Cleaned shape:", df_cleaned.shape)

num_cols_cleaned = df_cleaned.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15,10))
for i, col in enumerate(num_cols_cleaned, 1):
    plt.subplot(len(num_cols_cleaned)//2 + 1, 2, i)
    sns.boxplot(y=df_cleaned[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

dataset= pd.read_csv("bank_marketing_dataset.csv")
dataset.head()

dataset.isna().sum()

X = dataset.iloc[:, 2:4]        # job, marital
y = dataset.iloc[:, -1]        # deposit

y = y.map({'no': 0, 'yes': 1})

X = pd.get_dummies(X, drop_first=True)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,
    criterion="gini",
    max_depth=6,
    random_state=42
)

rf_model.fit(x_train, y_train)

y_pred = rf_model.predict(x_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd

# Load data
df = pd.read_csv("creditcard.csv")

# Preview data
df.head()

print(df.isna().sum())

X = df.drop("Class", axis=1).values
y = df["Class"].values

df['Class'].isna().sum()

df = df.dropna(subset=['Class'])

X = df.drop('Class', axis=1)
y = df['Class']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(
    criterion="gini",
    random_state=42
)
dt_model.fit(X_train_scaled, y_train)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=200,
    criterion="gini",
    random_state=42,
    class_weight="balanced"
)
rf_model.fit(X_train_scaled, y_train)

y_pred_dt = dt_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test_scaled)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Accuracy
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

# Confusion Matrices
print("Decision Tree Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("Random Forest Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

# Detailed report
print("\nDecision Tree Classification Report:\n", classification_report(y_test, y_pred_dt))
print("\nRandom Forest Classification Report:\n", classification_report(y_test, y_pred_rf))

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(
    criterion="gini",
    max_depth=4,        # limit depth for visualization
    random_state=42
)

dt_model.fit(X_train, y_train)
plt.figure(figsize=(30, 15))
plot_tree(
    dt_model,
    feature_names=df.drop("Class", axis=1).columns,
    class_names=["Normal", "Fraud"],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Decision Tree Visualization (Gini Index)")
plt.show()